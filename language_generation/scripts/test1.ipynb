{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf0908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 创建一个简单的数据集\n",
    "data = torch.randn(100, 3)  # 假设有100个样本，每个样本有3个特征\n",
    "dataset = TensorDataset(data)\n",
    "\n",
    "# 创建一个 DataLoader，设置 shuffle=False 以保持数据的原始顺序\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "for k in range(100):\n",
    "    # 从 DataLoader 中按顺序获取 batch\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # 这里的 batch 将按照索引顺序被加载\n",
    "        # i 为 0 时，你将得到第一个 batch，即索引为 0 到 7 的元素\n",
    "        # i 为 1 时，你将得到第二个 batch，即索引为 8 到 15 的元素\n",
    "        # 以此类推\n",
    "        indexed_batch = batch[0]\n",
    "        print(f'Batch {i}: {indexed_batch}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9b62b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, Batch 0: tensor([2, 0, 1])\n",
      "0, Batch 1: tensor([3, 4])\n",
      "1, Batch 0: tensor([2, 1, 3])\n",
      "1, Batch 1: tensor([4, 0])\n",
      "2, Batch 0: tensor([1, 3, 2])\n",
      "2, Batch 1: tensor([0, 4])\n",
      "3, Batch 0: tensor([3, 4, 1])\n",
      "3, Batch 1: tensor([2, 0])\n",
      "4, Batch 0: tensor([4, 3, 0])\n",
      "4, Batch 1: tensor([2, 1])\n",
      "5, Batch 0: tensor([1, 4, 0])\n",
      "5, Batch 1: tensor([2, 3])\n",
      "6, Batch 0: tensor([2, 3, 4])\n",
      "6, Batch 1: tensor([0, 1])\n",
      "7, Batch 0: tensor([3, 0, 4])\n",
      "7, Batch 1: tensor([2, 1])\n",
      "8, Batch 0: tensor([3, 0, 4])\n",
      "8, Batch 1: tensor([2, 1])\n",
      "9, Batch 0: tensor([3, 4, 0])\n",
      "9, Batch 1: tensor([2, 1])\n",
      "10, Batch 0: tensor([4, 0, 2])\n",
      "10, Batch 1: tensor([1, 3])\n",
      "11, Batch 0: tensor([0, 1, 4])\n",
      "11, Batch 1: tensor([3, 2])\n",
      "12, Batch 0: tensor([1, 0, 2])\n",
      "12, Batch 1: tensor([3, 4])\n",
      "13, Batch 0: tensor([3, 2, 4])\n",
      "13, Batch 1: tensor([1, 0])\n",
      "14, Batch 0: tensor([3, 1, 2])\n",
      "14, Batch 1: tensor([0, 4])\n",
      "15, Batch 0: tensor([0, 2, 1])\n",
      "15, Batch 1: tensor([3, 4])\n",
      "16, Batch 0: tensor([3, 1, 4])\n",
      "16, Batch 1: tensor([0, 2])\n",
      "17, Batch 0: tensor([1, 0, 4])\n",
      "17, Batch 1: tensor([2, 3])\n",
      "18, Batch 0: tensor([2, 3, 0])\n",
      "18, Batch 1: tensor([1, 4])\n",
      "19, Batch 0: tensor([1, 0, 3])\n",
      "19, Batch 1: tensor([2, 4])\n",
      "20, Batch 0: tensor([4, 0, 3])\n",
      "20, Batch 1: tensor([1, 2])\n",
      "21, Batch 0: tensor([4, 2, 0])\n",
      "21, Batch 1: tensor([1, 3])\n",
      "22, Batch 0: tensor([0, 1, 4])\n",
      "22, Batch 1: tensor([3, 2])\n",
      "23, Batch 0: tensor([4, 0, 3])\n",
      "23, Batch 1: tensor([1, 2])\n",
      "24, Batch 0: tensor([3, 4, 2])\n",
      "24, Batch 1: tensor([1, 0])\n",
      "25, Batch 0: tensor([2, 4, 0])\n",
      "25, Batch 1: tensor([3, 1])\n",
      "26, Batch 0: tensor([4, 3, 0])\n",
      "26, Batch 1: tensor([1, 2])\n",
      "27, Batch 0: tensor([2, 4, 1])\n",
      "27, Batch 1: tensor([0, 3])\n",
      "28, Batch 0: tensor([2, 0, 4])\n",
      "28, Batch 1: tensor([3, 1])\n",
      "29, Batch 0: tensor([0, 4, 3])\n",
      "29, Batch 1: tensor([1, 2])\n",
      "30, Batch 0: tensor([0, 3, 2])\n",
      "30, Batch 1: tensor([1, 4])\n",
      "31, Batch 0: tensor([0, 2, 1])\n",
      "31, Batch 1: tensor([3, 4])\n",
      "32, Batch 0: tensor([2, 3, 4])\n",
      "32, Batch 1: tensor([0, 1])\n",
      "33, Batch 0: tensor([2, 3, 1])\n",
      "33, Batch 1: tensor([4, 0])\n",
      "34, Batch 0: tensor([2, 3, 1])\n",
      "34, Batch 1: tensor([4, 0])\n",
      "35, Batch 0: tensor([0, 2, 3])\n",
      "35, Batch 1: tensor([1, 4])\n",
      "36, Batch 0: tensor([4, 2, 1])\n",
      "36, Batch 1: tensor([0, 3])\n",
      "37, Batch 0: tensor([4, 1, 2])\n",
      "37, Batch 1: tensor([0, 3])\n",
      "38, Batch 0: tensor([0, 1, 3])\n",
      "38, Batch 1: tensor([2, 4])\n",
      "39, Batch 0: tensor([4, 2, 0])\n",
      "39, Batch 1: tensor([3, 1])\n",
      "40, Batch 0: tensor([4, 0, 2])\n",
      "40, Batch 1: tensor([1, 3])\n",
      "41, Batch 0: tensor([2, 4, 1])\n",
      "41, Batch 1: tensor([3, 0])\n",
      "42, Batch 0: tensor([4, 2, 1])\n",
      "42, Batch 1: tensor([0, 3])\n",
      "43, Batch 0: tensor([0, 2, 1])\n",
      "43, Batch 1: tensor([4, 3])\n",
      "44, Batch 0: tensor([3, 0, 2])\n",
      "44, Batch 1: tensor([1, 4])\n",
      "45, Batch 0: tensor([0, 3, 4])\n",
      "45, Batch 1: tensor([1, 2])\n",
      "46, Batch 0: tensor([0, 2, 1])\n",
      "46, Batch 1: tensor([4, 3])\n",
      "47, Batch 0: tensor([3, 2, 4])\n",
      "47, Batch 1: tensor([1, 0])\n",
      "48, Batch 0: tensor([3, 0, 4])\n",
      "48, Batch 1: tensor([1, 2])\n",
      "49, Batch 0: tensor([4, 3, 1])\n",
      "49, Batch 1: tensor([0, 2])\n",
      "50, Batch 0: tensor([4, 1, 3])\n",
      "50, Batch 1: tensor([2, 0])\n",
      "51, Batch 0: tensor([2, 0, 1])\n",
      "51, Batch 1: tensor([3, 4])\n",
      "52, Batch 0: tensor([4, 2, 3])\n",
      "52, Batch 1: tensor([1, 0])\n",
      "53, Batch 0: tensor([3, 4, 1])\n",
      "53, Batch 1: tensor([2, 0])\n",
      "54, Batch 0: tensor([4, 3, 1])\n",
      "54, Batch 1: tensor([2, 0])\n",
      "55, Batch 0: tensor([1, 2, 3])\n",
      "55, Batch 1: tensor([0, 4])\n",
      "56, Batch 0: tensor([3, 4, 0])\n",
      "56, Batch 1: tensor([2, 1])\n",
      "57, Batch 0: tensor([0, 1, 4])\n",
      "57, Batch 1: tensor([2, 3])\n",
      "58, Batch 0: tensor([3, 0, 4])\n",
      "58, Batch 1: tensor([1, 2])\n",
      "59, Batch 0: tensor([3, 1, 4])\n",
      "59, Batch 1: tensor([2, 0])\n",
      "60, Batch 0: tensor([4, 2, 3])\n",
      "60, Batch 1: tensor([0, 1])\n",
      "61, Batch 0: tensor([1, 3, 4])\n",
      "61, Batch 1: tensor([2, 0])\n",
      "62, Batch 0: tensor([0, 3, 2])\n",
      "62, Batch 1: tensor([1, 4])\n",
      "63, Batch 0: tensor([1, 2, 3])\n",
      "63, Batch 1: tensor([4, 0])\n",
      "64, Batch 0: tensor([3, 0, 2])\n",
      "64, Batch 1: tensor([1, 4])\n",
      "65, Batch 0: tensor([1, 3, 2])\n",
      "65, Batch 1: tensor([4, 0])\n",
      "66, Batch 0: tensor([0, 1, 4])\n",
      "66, Batch 1: tensor([2, 3])\n",
      "67, Batch 0: tensor([4, 0, 2])\n",
      "67, Batch 1: tensor([3, 1])\n",
      "68, Batch 0: tensor([1, 3, 4])\n",
      "68, Batch 1: tensor([0, 2])\n",
      "69, Batch 0: tensor([0, 1, 2])\n",
      "69, Batch 1: tensor([3, 4])\n",
      "70, Batch 0: tensor([3, 1, 0])\n",
      "70, Batch 1: tensor([4, 2])\n",
      "71, Batch 0: tensor([2, 4, 0])\n",
      "71, Batch 1: tensor([3, 1])\n",
      "72, Batch 0: tensor([2, 3, 0])\n",
      "72, Batch 1: tensor([1, 4])\n",
      "73, Batch 0: tensor([2, 1, 0])\n",
      "73, Batch 1: tensor([4, 3])\n",
      "74, Batch 0: tensor([2, 1, 4])\n",
      "74, Batch 1: tensor([0, 3])\n",
      "75, Batch 0: tensor([0, 3, 4])\n",
      "75, Batch 1: tensor([2, 1])\n",
      "76, Batch 0: tensor([3, 4, 1])\n",
      "76, Batch 1: tensor([2, 0])\n",
      "77, Batch 0: tensor([3, 2, 0])\n",
      "77, Batch 1: tensor([4, 1])\n",
      "78, Batch 0: tensor([4, 2, 1])\n",
      "78, Batch 1: tensor([3, 0])\n",
      "79, Batch 0: tensor([1, 3, 0])\n",
      "79, Batch 1: tensor([2, 4])\n",
      "80, Batch 0: tensor([4, 0, 2])\n",
      "80, Batch 1: tensor([1, 3])\n",
      "81, Batch 0: tensor([4, 0, 3])\n",
      "81, Batch 1: tensor([2, 1])\n",
      "82, Batch 0: tensor([3, 0, 4])\n",
      "82, Batch 1: tensor([1, 2])\n",
      "83, Batch 0: tensor([0, 1, 2])\n",
      "83, Batch 1: tensor([4, 3])\n",
      "84, Batch 0: tensor([2, 1, 4])\n",
      "84, Batch 1: tensor([3, 0])\n",
      "85, Batch 0: tensor([3, 4, 1])\n",
      "85, Batch 1: tensor([0, 2])\n",
      "86, Batch 0: tensor([1, 4, 0])\n",
      "86, Batch 1: tensor([3, 2])\n",
      "87, Batch 0: tensor([1, 3, 0])\n",
      "87, Batch 1: tensor([4, 2])\n",
      "88, Batch 0: tensor([0, 1, 3])\n",
      "88, Batch 1: tensor([2, 4])\n",
      "89, Batch 0: tensor([3, 1, 0])\n",
      "89, Batch 1: tensor([4, 2])\n",
      "90, Batch 0: tensor([1, 0, 4])\n",
      "90, Batch 1: tensor([3, 2])\n",
      "91, Batch 0: tensor([0, 2, 4])\n",
      "91, Batch 1: tensor([1, 3])\n",
      "92, Batch 0: tensor([3, 2, 4])\n",
      "92, Batch 1: tensor([0, 1])\n",
      "93, Batch 0: tensor([2, 0, 4])\n",
      "93, Batch 1: tensor([1, 3])\n",
      "94, Batch 0: tensor([0, 2, 1])\n",
      "94, Batch 1: tensor([3, 4])\n",
      "95, Batch 0: tensor([3, 0, 2])\n",
      "95, Batch 1: tensor([4, 1])\n",
      "96, Batch 0: tensor([3, 2, 0])\n",
      "96, Batch 1: tensor([1, 4])\n",
      "97, Batch 0: tensor([0, 4, 3])\n",
      "97, Batch 1: tensor([1, 2])\n",
      "98, Batch 0: tensor([0, 3, 4])\n",
      "98, Batch 1: tensor([2, 1])\n",
      "99, Batch 0: tensor([3, 4, 2])\n",
      "99, Batch 1: tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "\n",
    "class Splited_FMRI_dataset(Dataset):\n",
    "    def __init__(self,):\n",
    "        self.inputs = list(range(1000))\n",
    "        self.most_epoch = 5\n",
    "    def __len__(self):\n",
    "        if self.most_epoch > -1:\n",
    "            return min(self.most_epoch, len(self.inputs))\n",
    "        return len(self.inputs)\n",
    "    def __getitem__(self, idx):\n",
    "        input_sample = self.inputs[idx]\n",
    "        return input_sample\n",
    "dataset = Splited_FMRI_dataset()\n",
    "\n",
    "for k in range(100):\n",
    "    random.shuffle(dataset.inputs)\n",
    "    dataloader = DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        print(f'{k}, Batch {i}: {batch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea841541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoTokenizer, AutoModel, LlamaForCausalLM, LlamaTokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('/home/yzy/.cache/huggingface/hub/models--gpt2-large/snapshots/97935fc1a406f447320c3db70fe9e9875dca2595')\n",
    "model = GPT2LMHeadModel.from_pretrained('/home/yzy/.cache/huggingface/hub/models--gpt2-large/snapshots/97935fc1a406f447320c3db70fe9e9875dca2595')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8d9f1426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|> the best I've seen in a long time. It's been so hot and humid that it feels like we're living on Mars.\"\n",
      "\n",
      "The weather\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "content_true1 = 'Beijing is'\n",
    "content_true1 = 'The weather today is'\n",
    "input_ids1 = tokenizer.encode_plus(content_true1, max_length=32, truncation=False, return_tensors='pt',padding='max_length')\n",
    "\n",
    "content_prev1 = model.transformer.wte(input_ids1['input_ids'])\n",
    "\n",
    "padding_counts = (input_ids1['attention_mask'] == 0).sum(dim=1)\n",
    "\n",
    "# 初始化新的张量\n",
    "front_padded_input_embeds = torch.zeros_like(content_prev1)\n",
    "front_padded_mask = torch.zeros_like(input_ids1['attention_mask'])\n",
    "\n",
    "for i in range(content_prev1.size(0)):  # 遍历每个样本\n",
    "    # 计算需要移动的位置数\n",
    "    shift = padding_counts[i].item()\n",
    "    \n",
    "    # 为 input_embeds 和 mask 重新排列\n",
    "    front_padded_input_embeds[i, content_prev1.size(1) - shift:] = content_prev1[i, :shift]\n",
    "    front_padded_mask[i, content_prev1.size(1) - shift:] = input_ids1['attention_mask'][i, :shift]\n",
    "\n",
    "content_prev1 = front_padded_input_embeds\n",
    "input_ids1['attention_mask'] = front_padded_mask\n",
    "\n",
    "input_ids2 = tokenizer.encode_plus(content_true2, max_length=32, truncation=False, return_tensors='pt',padding='max_length')\n",
    "\n",
    "\n",
    "# content_prev2 = model.transformer.wte(input_ids2['input_ids'])\n",
    "content_prev = torch.cat([content_prev1, ], dim=0)\n",
    "# model.generate(inputs_embeds = content_prev, min_length = len(content_prev[0])+4, max_length=len(content_prev[0])+32,return_dict_in_generate=True,num_beams=1,do_sample=False,attention_mask=torch.ones(content_prev.shape[:-1], dtype=torch.long, device=self.device), pad_token_id=self.tokenizer.eos_token_id)\n",
    "content_prev_mask = torch.cat([input_ids1['attention_mask'], ], dim=0)\n",
    "\n",
    "# re1 = model.generate(inputs_embeds = content_prev, attention_mask = content_prev_mask,return_dict_in_generate=True,min_length = 10, max_length=32,num_beams=1,do_sample=False, repetition_penalty=2.0, pad_token_id=tokenizer.eos_token_id)\n",
    "re1 = model.generate(inputs_embeds = content_prev, attention_mask = content_prev_mask,return_dict_in_generate=True,min_length = 10, max_length=32,do_sample=False, repetition_penalty=2.0, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "re = [tokenizer.convert_ids_to_tokens(re1['sequences'][0]), ]\n",
    "print(tokenizer.convert_tokens_to_string(re[0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "af7e66de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_prev1.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "997116f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids1 = tokenizer.encode_plus(content_true1, max_length=32, truncation=False, return_tensors='pt',padding='max_length')\n",
    "input_ids1['input_ids'] = torch.cat([torch.tensor([tokenizer.pad_token_id] * padding_length), input_ids1['input_ids'][0]]).long()\n",
    "input_ids1['attention_mask'] = torch.cat([torch.tensor([0] * padding_length), input_ids1['attention_mask'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e2fc573c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids1['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bf1c7ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.tensor([0] * padding_length), input_ids1['attention_mask'][0]]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4d8d9226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1280])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_prev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6e39bd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_prev_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "618b76b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=model.generate(inputs_embeds = content_prev, attention_mask = content_prev_mask, min_length = 10, max_length=32,return_dict_in_generate=True,num_beams=5,do_sample=False, repetition_penalty=2.0,pad_token_id=tokenizer.eos_token_id)\n",
    "content_prev = content_prev[content_prev_mask==1].unsqueeze(0)\n",
    "b=model.generate(inputs_embeds =  content_prev, attention_mask = torch.ones(content_prev.shape[:-1], dtype=torch.long,), min_length = 10, max_length=32,return_dict_in_generate=True,num_beams=5,do_sample=False, repetition_penalty=2.0,pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3944190f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BeamSearchDecoderOnlyOutput(sequences=tensor([[50256,   464,   995,   338,   749, 44042,  1748,    11,   351,   257,\n",
      "          3265,   286,   517,   621,   352,    13,    18,  2997,   661,    13,\n",
      "           632,   318,   635,  1363,   284,   262,   995,   338,  1218,    12,\n",
      "         28209,  3773]]), sequences_scores=None, scores=None, beam_indices=None, attentions=None, hidden_states=None)\n",
      "<|endoftext|>The world's most populous city, with a population of more than 1.3 billion people. It is also home to the world's second-largest economy\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(a['sequences'][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c79e9cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BeamSearchDecoderOnlyOutput(sequences=tensor([[50256,  1849,  1169,   691,  1499,   287,   262,   995,   326,   468,\n",
      "           407, 39067,   262, 11680,   319,   262, 27405,  1883,   286,  1439,\n",
      "         39196,   286, 49298,  1028,  6926,   357,    34,  1961, 12298,   737,\n",
      "           220,  1849]]), sequences_scores=None, scores=None, beam_indices=None, attentions=None, hidden_states=None)\n",
      "<|endoftext|> the only country in the world that has not ratified the Convention on the Elimination of All Forms of Discrimination against Women (CEDAW).  \n"
     ]
    }
   ],
   "source": [
    "print(b)\n",
    "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(b['sequences'][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e467f7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 1280])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_prev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5683be33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb32349b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25017f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(re1['sequences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a84f5379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method generate in module transformers.generation.utils:\n",
      "\n",
      "generate(inputs: Union[torch.Tensor, NoneType] = None, generation_config: Union[transformers.generation.configuration_utils.GenerationConfig, NoneType] = None, logits_processor: Union[transformers.generation.logits_process.LogitsProcessorList, NoneType] = None, stopping_criteria: Union[transformers.generation.stopping_criteria.StoppingCriteriaList, NoneType] = None, prefix_allowed_tokens_fn: Union[Callable[[int, torch.Tensor], List[int]], NoneType] = None, synced_gpus: Union[bool, NoneType] = None, assistant_model: Union[ForwardRef('PreTrainedModel'), NoneType] = None, streamer: Union[ForwardRef('BaseStreamer'), NoneType] = None, **kwargs) -> Union[transformers.generation.utils.GreedySearchEncoderDecoderOutput, transformers.generation.utils.GreedySearchDecoderOnlyOutput, transformers.generation.utils.SampleEncoderDecoderOutput, transformers.generation.utils.SampleDecoderOnlyOutput, transformers.generation.utils.BeamSearchEncoderDecoderOutput, transformers.generation.utils.BeamSearchDecoderOnlyOutput, transformers.generation.utils.BeamSampleEncoderDecoderOutput, transformers.generation.utils.BeamSampleDecoderOnlyOutput, transformers.generation.utils.ContrastiveSearchEncoderDecoderOutput, transformers.generation.utils.ContrastiveSearchDecoderOnlyOutput, torch.LongTensor] method of transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel instance\n",
      "    Generates sequences of token ids for models with a language modeling head.\n",
      "    \n",
      "    <Tip warning={true}>\n",
      "    \n",
      "    Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the\n",
      "    model's default generation configuration. You can override any `generation_config` by passing the corresponding\n",
      "    parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n",
      "    \n",
      "    For an overview of generation strategies and code examples, check out the [following\n",
      "    guide](../generation_strategies).\n",
      "    \n",
      "    </Tip>\n",
      "    \n",
      "    Parameters:\n",
      "        inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
      "            The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
      "            method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
      "            should of in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
      "            `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
      "        generation_config (`~generation.GenerationConfig`, *optional*):\n",
      "            The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n",
      "            passed to generate matching the attributes of `generation_config` will override them. If\n",
      "            `generation_config` is not provided, the default will be used, which had the following loading\n",
      "            priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n",
      "            configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n",
      "            default values, whose documentation should be checked to parameterize generation.\n",
      "        logits_processor (`LogitsProcessorList`, *optional*):\n",
      "            Custom logits processors that complement the default logits processors built from arguments and\n",
      "            generation config. If a logit processor is passed that is already created with the arguments or a\n",
      "            generation config an error is thrown. This feature is intended for advanced users.\n",
      "        stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      "            Custom stopping criteria that complement the default stopping criteria built from arguments and a\n",
      "            generation config. If a stopping criteria is passed that is already created with the arguments or a\n",
      "            generation config an error is thrown. This feature is intended for advanced users.\n",
      "        prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
      "            If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
      "            provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
      "            `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
      "            on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
      "            for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
      "            Retrieval](https://arxiv.org/abs/2010.00904).\n",
      "        synced_gpus (`bool`, *optional*):\n",
      "            Whether to continue running the while loop until max_length. Unless overridden this flag will be set to\n",
      "            `True` under DeepSpeed ZeRO Stage 3 multiple GPUs environment to avoid hanging if one GPU finished\n",
      "            generating before other GPUs. Otherwise it'll be set to `False`.\n",
      "        assistant_model (`PreTrainedModel`, *optional*):\n",
      "            An assistant model that can be used to accelerate generation. The assistant model must have the exact\n",
      "            same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistent model\n",
      "            is much faster than running generation with the model you're calling generate from. As such, the\n",
      "            assistant model should be much smaller.\n",
      "        streamer (`BaseStreamer`, *optional*):\n",
      "            Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n",
      "            through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n",
      "        kwargs:\n",
      "            Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n",
      "            forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n",
      "            specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n",
      "    \n",
      "    Return:\n",
      "        [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
      "        or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor`.\n",
      "    \n",
      "            If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
      "            [`~utils.ModelOutput`] types are:\n",
      "    \n",
      "                - [`~generation.GreedySearchDecoderOnlyOutput`],\n",
      "                - [`~generation.SampleDecoderOnlyOutput`],\n",
      "                - [`~generation.BeamSearchDecoderOnlyOutput`],\n",
      "                - [`~generation.BeamSampleDecoderOnlyOutput`]\n",
      "    \n",
      "            If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
      "            [`~utils.ModelOutput`] types are:\n",
      "    \n",
      "                - [`~generation.GreedySearchEncoderDecoderOutput`],\n",
      "                - [`~generation.SampleEncoderDecoderOutput`],\n",
      "                - [`~generation.BeamSearchEncoderDecoderOutput`],\n",
      "                - [`~generation.BeamSampleEncoderDecoderOutput`]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7b5cf16",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mre1\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/transformers/tokenization_utils.py:906\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    904\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m--> 906\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_ids:\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(re1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b66304da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 1280])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_prev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3fe2250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f111af58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids['attention_mask']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
